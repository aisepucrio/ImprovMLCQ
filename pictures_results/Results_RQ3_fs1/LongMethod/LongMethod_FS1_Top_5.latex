\begin{tabular}{llllllllllllll}
\toprule
Model & Parameters & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Mean & Std \\
\midrule
ExtraTreesClassifier & {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 5983, 'verbose': 0, 'warm_start': False} & [0.6334, 0.835, 0.8555, 0.1776, 0.2942, 0.1717] & [0.6407, 0.83, 0.8672, 0.1823, 0.3012, 0.1803] & [0.6421, 0.8358, 0.8359, 0.1786, 0.2944, 0.1726] & [0.6495, 0.8524, 0.9062, 0.1913, 0.3159, 0.1975] & [0.8001, 0.9241, 0.9805, 0.3065, 0.467, 0.383] & [0.6071, 0.8442, 0.8902, 0.1713, 0.2873, 0.1623] & [0.6364, 0.8429, 0.8471, 0.1772, 0.2931, 0.1711] & [0.6336, 0.8347, 0.8706, 0.1792, 0.2972, 0.1755] & [0.7948, 0.9233, 0.9686, 0.2987, 0.4566, 0.371] & [0.6654, 0.8483, 0.8867, 0.1962, 0.3213, 0.205] & [0.6703, 0.8571, 0.8909, 0.2059, 0.3328, 0.219] & [0.0651, 0.0339, 0.0464, 0.0489, 0.0653, 0.08] \\
XGBClassifier & {'objective': 'binary:logistic', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': 'cpu', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 5983, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0} & [0.6627, 0.8758, 0.9727, 0.206, 0.3399, 0.2258] & [0.662, 0.8514, 0.9727, 0.2056, 0.3395, 0.2253] & [0.662, 0.8792, 0.9844, 0.2071, 0.3422, 0.2283] & [0.6732, 0.8744, 0.9727, 0.2112, 0.347, 0.2348] & [0.8657, 0.9269, 0.9922, 0.3987, 0.5689, 0.5059] & [0.665, 0.8695, 0.9725, 0.2065, 0.3407, 0.2272] & [0.6556, 0.8777, 0.9725, 0.202, 0.3345, 0.2194] & [0.6458, 0.8713, 0.9529, 0.195, 0.3238, 0.2066] & [0.6577, 0.8594, 0.9686, 0.2025, 0.3349, 0.2201] & [0.6609, 0.8704, 0.9609, 0.2036, 0.3361, 0.2213] & [0.6811, 0.8756, 0.9722, 0.2238, 0.3607, 0.2515] & [0.0619, 0.0189, 0.0103, 0.0584, 0.0696, 0.0851] \\
LGBMClassifier & {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 5983, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0} & [0.7175, 0.8815, 0.8828, 0.2247, 0.3582, 0.2516] & [0.8472, 0.9106, 0.9062, 0.3591, 0.5144, 0.4432] & [0.7217, 0.8835, 0.8672, 0.2252, 0.3575, 0.2513] & [0.7189, 0.8784, 0.8594, 0.2222, 0.3531, 0.2462] & [0.7213, 0.8704, 0.8398, 0.221, 0.3499, 0.2428] & [0.7181, 0.8746, 0.8431, 0.2187, 0.3473, 0.2399] & [0.7094, 0.8796, 0.8471, 0.2139, 0.3415, 0.2325] & [0.7114, 0.867, 0.8471, 0.2151, 0.3431, 0.2345] & [0.7111, 0.8603, 0.8196, 0.2109, 0.3355, 0.2259] & [0.7258, 0.8746, 0.8477, 0.2251, 0.3557, 0.2498] & [0.7302, 0.8781, 0.856, 0.2336, 0.3656, 0.2618] & [0.0393, 0.0127, 0.0232, 0.0421, 0.0501, 0.061] \\
KNeighborsClassifier & {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'manhattan', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 19, 'p': 2, 'weights': 'uniform'} & [0.8804, 0.9239, 0.7148, 0.404, 0.5162, 0.4539] & [0.8723, 0.8695, 0.5938, 0.3671, 0.4537, 0.386] & [0.8905, 0.9028, 0.6055, 0.4212, 0.4968, 0.4376] & [0.8943, 0.8728, 0.6367, 0.437, 0.5183, 0.4612] & [0.888, 0.8574, 0.6211, 0.4151, 0.4977, 0.4374] & [0.88, 0.87, 0.6588, 0.3953, 0.4941, 0.4308] & [0.8939, 0.8761, 0.6588, 0.4364, 0.525, 0.4681] & [0.8894, 0.8645, 0.6392, 0.4201, 0.507, 0.4477] & [0.8849, 0.8571, 0.6314, 0.4055, 0.4939, 0.4324] & [0.8863, 0.8598, 0.6172, 0.4093, 0.4922, 0.4311] & [0.886, 0.8754, 0.6377, 0.4111, 0.4995, 0.4386] & [0.0065, 0.0205, 0.0325, 0.0194, 0.0189, 0.0215] \\
RandomForestClassifier & {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': {}, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 0.8013559603351242, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 4.3574147757201524e-08, 'min_samples_leaf': 5, 'min_samples_split': 7, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 24, 'n_jobs': -1, 'oob_score': False, 'random_state': 5983, 'verbose': 0, 'warm_start': False} & [0.8448, 0.9265, 0.9922, 0.3644, 0.5331, 0.4629] & [0.6812, 0.8746, 0.9258, 0.2094, 0.3415, 0.2292] & [0.6798, 0.8783, 0.9258, 0.2086, 0.3405, 0.228] & [0.6822, 0.8739, 0.9062, 0.2073, 0.3375, 0.2248] & [0.8403, 0.9279, 0.9961, 0.3581, 0.5269, 0.4553] & [0.6724, 0.8714, 0.9137, 0.2026, 0.3317, 0.2177] & [0.6734, 0.879, 0.9216, 0.2042, 0.3343, 0.2208] & [0.8297, 0.9201, 0.9843, 0.3415, 0.5071, 0.432] & [0.6828, 0.8821, 0.9255, 0.2096, 0.3418, 0.2301] & [0.8332, 0.9278, 0.9844, 0.3471, 0.5132, 0.4392] & [0.742, 0.8962, 0.9476, 0.2653, 0.4107, 0.314] & [0.0777, 0.0243, 0.0347, 0.0717, 0.0895, 0.1092] \\
\bottomrule
\end{tabular}
