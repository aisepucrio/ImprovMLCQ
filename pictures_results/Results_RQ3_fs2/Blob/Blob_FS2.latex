\begin{tabular}{llllllllllllll}
\toprule
Model & Parameters & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Mean & Std \\
\midrule
XGBClassifier & {'objective': 'binary:logistic', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.9664870633705891, 'device': 'cpu', 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.4722317441963545, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 10, 'max_leaves': None, 'min_child_weight': 2, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 260, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': 4821, 'reg_alpha': 0.020423157693100564, 'reg_lambda': 0.012845168082626845, 'sampling_method': None, 'scale_pos_weight': 34.98246551901251, 'subsample': 0.547931642153044, 'tree_method': 'auto', 'validate_parameters': None, 'verbosity': 0} & [0.8287, 0.9026, 0.9721, 0.5052, 0.6648, 0.5648] & [0.8249, 0.9092, 0.978, 0.4995, 0.6613, 0.5593] & [0.8389, 0.9185, 0.9741, 0.5213, 0.6792, 0.5844] & [0.8123, 0.8925, 0.9681, 0.4821, 0.6437, 0.535] & [0.8225, 0.9015, 0.99, 0.4965, 0.6613, 0.5583] & [0.8332, 0.9087, 0.9741, 0.5121, 0.6713, 0.5735] & [0.8283, 0.9119, 0.9641, 0.5047, 0.6626, 0.5621] & [0.8158, 0.8986, 0.9581, 0.4863, 0.6452, 0.538] & [0.828, 0.9106, 0.9741, 0.5041, 0.6644, 0.5639] & [0.8133, 0.9016, 0.9661, 0.483, 0.644, 0.5359] & [0.8246, 0.9056, 0.9719, 0.4995, 0.6598, 0.5575] & [0.0082, 0.0072, 0.0083, 0.0121, 0.0113, 0.0157] \\
LGBMClassifier & {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.004962925408787576, 'max_depth': -1, 'min_child_samples': 2, 'min_child_weight': 0.001, 'min_split_gain': 0.9391077448536558, 'n_estimators': 240, 'n_jobs': -1, 'num_leaves': 194, 'objective': None, 'random_state': 4821, 'reg_alpha': 3.373337139078863e-10, 'reg_lambda': 3.55500029963111e-06, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0, 'feature_fraction': 0.6111486076973656, 'bagging_fraction': 0.7826806361159054, 'bagging_freq': 2} & [0.8256, 0.9082, 0.9621, 0.5005, 0.6585, 0.5565] & [0.834, 0.9307, 0.9621, 0.5133, 0.6694, 0.5719] & [0.8354, 0.928, 0.9502, 0.5162, 0.669, 0.5719] & [0.8221, 0.9258, 0.9661, 0.4959, 0.6554, 0.5517] & [0.8333, 0.9255, 0.9781, 0.5125, 0.6726, 0.5749] & [0.8346, 0.9205, 0.9661, 0.5143, 0.6713, 0.5741] & [0.8371, 0.9254, 0.9561, 0.5184, 0.6723, 0.5762] & [0.8203, 0.915, 0.9661, 0.4929, 0.6527, 0.5481] & [0.8283, 0.9213, 0.9621, 0.5047, 0.6621, 0.5615] & [0.8074, 0.9058, 0.9601, 0.4748, 0.6354, 0.5241] & [0.8278, 0.9206, 0.9629, 0.5044, 0.6619, 0.5611] & [0.0088, 0.008, 0.0069, 0.0129, 0.0112, 0.0157] \\
RandomForestClassifier & {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 4821, 'verbose': 0, 'warm_start': False} & [0.7834, 0.917, 0.988, 0.4459, 0.6145, 0.4923] & [0.7799, 0.9156, 0.996, 0.4424, 0.6126, 0.489] & [0.7827, 0.9184, 0.996, 0.446, 0.6161, 0.4937] & [0.7639, 0.9218, 0.992, 0.4253, 0.5953, 0.4639] & [0.7865, 0.9213, 0.992, 0.4503, 0.6194, 0.4986] & [0.7882, 0.9167, 0.986, 0.4516, 0.6194, 0.4994] & [0.7805, 0.9152, 0.976, 0.4421, 0.6086, 0.4846] & [0.7739, 0.9097, 0.994, 0.4357, 0.6058, 0.4793] & [0.7879, 0.922, 0.99, 0.4513, 0.62, 0.4999] & [0.7565, 0.9085, 0.99, 0.4172, 0.587, 0.4523] & [0.7783, 0.9166, 0.99, 0.4408, 0.6099, 0.4853] & [0.0101, 0.0044, 0.0056, 0.011, 0.0105, 0.0152] \\
ExtraTreesClassifier & {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 4821, 'verbose': 0, 'warm_start': False} & [0.7761, 0.9104, 0.9481, 0.4354, 0.5967, 0.4697] & [0.7778, 0.9142, 0.9681, 0.4385, 0.6036, 0.4781] & [0.7841, 0.9236, 0.9542, 0.4456, 0.6075, 0.4844] & [0.7649, 0.9125, 0.9641, 0.4246, 0.5895, 0.4577] & [0.7757, 0.913, 0.9363, 0.4348, 0.5938, 0.4661] & [0.7784, 0.9068, 0.9481, 0.4382, 0.5994, 0.4735] & [0.7652, 0.9057, 0.9261, 0.4218, 0.5796, 0.4467] & [0.7589, 0.8985, 0.9361, 0.4158, 0.5758, 0.4403] & [0.7844, 0.9197, 0.9621, 0.4459, 0.6094, 0.4867] & [0.7652, 0.9037, 0.9461, 0.4232, 0.5848, 0.4526] & [0.7731, 0.9108, 0.9489, 0.4324, 0.594, 0.4656] & [0.0084, 0.0071, 0.0128, 0.0099, 0.0109, 0.015] \\
DecisionTreeClassifier & {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 16, 'max_features': 0.44373473698549964, 'max_leaf_nodes': None, 'min_impurity_decrease': 3.211895599192418e-05, 'min_samples_leaf': 3, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': 4821, 'splitter': 'best'} & [0.7942, 0.9111, 0.9481, 0.4572, 0.6169, 0.4987] & [0.8047, 0.9144, 0.9561, 0.471, 0.6311, 0.5183] & [0.7855, 0.9051, 0.9422, 0.4466, 0.606, 0.4833] & [0.7973, 0.9133, 0.9502, 0.4618, 0.6215, 0.5048] & [0.797, 0.9061, 0.9661, 0.4619, 0.625, 0.5086] & [0.807, 0.9117, 0.9521, 0.4742, 0.633, 0.5213] & [0.8126, 0.9122, 0.9421, 0.4816, 0.6374, 0.5283] & [0.799, 0.9104, 0.9521, 0.4636, 0.6235, 0.5078] & [0.8116, 0.9138, 0.9481, 0.4803, 0.6376, 0.5281] & [0.7851, 0.904, 0.9621, 0.4467, 0.6101, 0.4878] & [0.7994, 0.9102, 0.9519, 0.4645, 0.6242, 0.5087] & [0.0092, 0.0036, 0.0074, 0.0118, 0.0103, 0.0149] \\
\bottomrule
\end{tabular}
